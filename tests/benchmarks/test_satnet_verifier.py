"""
Tests for the SatNet Benchmark Verifier.

Validates the verifier against ground truth solutions generated from the 
reference NASA/JPL SatNet implementation, and checks error handling 
for crafted invalid scenarios.
"""

import json
import pytest
import numpy as np
from pathlib import Path
from dataclasses import dataclass
from copy import deepcopy

from benchmarks.satnet.verifier import (
    verify_files, 
    verify, 
    Instance, 
    Solution, 
    parse_problems, 
    parse_maintenance,
    Request,
    Track
)

# Paths
DATASET_DIR = Path("benchmarks/satnet/dataset")
FIXTURES_DIR = Path("tests/fixtures/satnet_mock_solutions")
PROBLEMS_JSON = DATASET_DIR / "problems.json"
MAINTENANCE_CSV = DATASET_DIR / "maintenance.csv"
GROUND_TRUTH_SUMMARY = FIXTURES_DIR / "ground_truth_summary.json"


@dataclass
class GroundTruthCase:
    week: int
    year: int
    solution_path: Path
    metrics_path: Path


def get_ground_truth_cases():
    """Load available ground truth cases from fixtures."""
    if not GROUND_TRUTH_SUMMARY.exists():
        return []

    with open(GROUND_TRUTH_SUMMARY, "r") as f:
        summary = json.load(f)

    cases = []
    for item in summary:
        cases.append(
            GroundTruthCase(
                week=item["week"],
                year=item["year"],
                solution_path=FIXTURES_DIR / item["solution_file"],
                metrics_path=FIXTURES_DIR / item["metrics_file"],
            )
        )
    return cases


@pytest.fixture
def load_ground_truth(request):
    """Fixture to load a specific ground truth case."""
    case = request.param
    
    # Load metrics
    with open(case.metrics_path, "r") as f:
        metrics = json.load(f)
        
    # Load solution
    with open(case.solution_path, "r") as f:
        solution_data = json.load(f)
        solution = Solution(tracks=[Track.from_dict(t) for t in solution_data])

    # Parse instance
    instance_requests = parse_problems(PROBLEMS_JSON, case.week, case.year)
    maintenance = parse_maintenance(MAINTENANCE_CSV, case.week, case.year)
    instance = Instance(
        week=case.week, 
        year=case.year, 
        requests=instance_requests, 
        maintenance=maintenance
    )

    return instance, solution, metrics


@pytest.mark.parametrize("load_ground_truth", get_ground_truth_cases(), indirect=True)
def test_ground_truth_validation(load_ground_truth):
    """
    Verify that the verifier correctly validates ground truth solutions 
    generated by the reference implementation.
    """
    instance, solution, metrics = load_ground_truth
    
    # Run verifier
    result = verify(instance, solution)
    
    # 1. Check validity
    assert result.is_valid, f"Ground truth solution for W{instance.week} should be valid. Errors: {result.errors}"
    
    # 2. Check score accuracy (total hours)
    # Using a small tolerance for floating point comparisons
    assert np.isclose(result.score, metrics["score"], atol=1e-4), \
        f"Score mismatch: computed {result.score}, expected {metrics['score']}"
        
    # 3. Check track count
    assert result.n_tracks == metrics["n_tracks"], \
        f"Track count mismatch: computed {result.n_tracks}, expected {metrics['n_tracks']}"
        
    # 4. Check satisfied requests count
    assert result.n_satisfied_requests == metrics["n_satisfied_requests"], \
        f"Satisfied request count mismatch: computed {result.n_satisfied_requests}, expected {metrics['n_satisfied_requests']}"

    # 5. Check fairness metrics (U_rms, U_max)
    assert np.isclose(result.u_rms, metrics["u_rms"], atol=1e-4), \
        f"U_rms mismatch: computed {result.u_rms}, expected {metrics['u_rms']}"

    assert np.isclose(result.u_max, metrics["u_max"], atol=1e-4), \
        f"U_max mismatch: computed {result.u_max}, expected {metrics['u_max']}"

    # 6. Check per-mission U_i values
    expected_per_mission_u_i = metrics["per_mission_u_i"]
    assert len(result.per_mission_u_i) == len(expected_per_mission_u_i), \
        f"Per-mission U_i count mismatch: computed {len(result.per_mission_u_i)}, expected {len(expected_per_mission_u_i)}"

    for mission_id, expected_u_i in expected_per_mission_u_i.items():
        assert mission_id in result.per_mission_u_i, \
            f"Mission {mission_id} missing in computed per_mission_u_i"
        assert np.isclose(result.per_mission_u_i[mission_id], expected_u_i, atol=1e-4), \
            f"U_i mismatch for mission {mission_id}: computed {result.per_mission_u_i[mission_id]}, expected {expected_u_i}"


@pytest.fixture
def simple_valid_case():
    """Load a single valid case for corruption testing (using W10_2018)."""
    week, year = 10, 2018
    solution_path = FIXTURES_DIR / "W10_2018_solution.json"
    
    if not solution_path.exists():
        pytest.skip("W10_2018 fixture not found")

    with open(solution_path, "r") as f:
        solution_data = json.load(f)
        solution = Solution(tracks=[Track.from_dict(t) for t in solution_data])

    instance_requests = parse_problems(PROBLEMS_JSON, week, year)
    maintenance = parse_maintenance(MAINTENANCE_CSV, week, year)
    instance = Instance(
        week=week, 
        year=year, 
        requests=instance_requests, 
        maintenance=maintenance
    )
    
    return instance, solution


def test_violation_view_period(simple_valid_case):
    """Test detection of tracks scheduled outside valid View Periods."""
    instance, valid_solution = simple_valid_case
    
    # Create a corrupt solution
    corrupt_tracks = deepcopy(valid_solution.tracks)
    target_track = corrupt_tracks[0]
    
    # Shift time window by 1 week (guaranteed to be outside VP for this week)
    shift = 7 * 24 * 3600
    target_track.start_time += shift
    target_track.tracking_on += shift
    target_track.tracking_off += shift
    target_track.end_time += shift
    
    corrupt_solution = Solution(tracks=corrupt_tracks)
    result = verify(instance, corrupt_solution)
    
    assert not result.is_valid
    assert any("not within any View Period" in e for e in result.errors)


def test_violation_overlap(simple_valid_case):
    """Test detection of overlapping tracks on the same antenna."""
    instance, valid_solution = simple_valid_case
    
    # Create a corrupt solution by duplicating the first track
    # This creates a perfect overlap
    corrupt_tracks = deepcopy(valid_solution.tracks)
    corrupt_tracks.append(deepcopy(corrupt_tracks[0]))
    
    corrupt_solution = Solution(tracks=corrupt_tracks)
    result = verify(instance, corrupt_solution)
    
    assert not result.is_valid
    assert any("Overlap between tracks" in e for e in result.errors)


def test_violation_setup_time_mismatch(simple_valid_case):
    """Test detection of inconsistent setup times."""
    instance, valid_solution = simple_valid_case
    
    corrupt_tracks = deepcopy(valid_solution.tracks)
    target_track = corrupt_tracks[0]
    
    # Alter start_time so it doesn't match tracking_on - setup_time
    # Request setup time is fixed, so this creates an inconsistency
    target_track.start_time -= 60  # Shift back 1 minute
    
    corrupt_solution = Solution(tracks=corrupt_tracks)
    result = verify(instance, corrupt_solution)
    
    assert not result.is_valid
    assert any("Setup time mismatch" in e for e in result.errors)


def test_violation_teardown_time_mismatch(simple_valid_case):
    """Test detection of inconsistent teardown times."""
    instance, valid_solution = simple_valid_case
    
    corrupt_tracks = deepcopy(valid_solution.tracks)
    target_track = corrupt_tracks[0]
    
    # Alter end_time so it doesn't match tracking_off + teardown_time
    target_track.end_time += 60  # Extend by 1 minute
    
    corrupt_solution = Solution(tracks=corrupt_tracks)
    result = verify(instance, corrupt_solution)
    
    assert not result.is_valid
    assert any("Teardown time mismatch" in e for e in result.errors)


def test_violation_minimum_duration(simple_valid_case):
    """Test detection of tracks shorter than minimum duration."""
    instance, valid_solution = simple_valid_case
    
    corrupt_tracks = deepcopy(valid_solution.tracks)
    target_track = corrupt_tracks[0]
    request = instance.requests[target_track.track_id]
    
    # Make track 1 second shorter than minimum
    # duration_min is in hours
    min_duration_sec = int(request.duration_min * 3600)
    
    # Set duration to be extremely short (1 second) to be safe
    # We can't easily adjust to exactly min - 1s because we need to satisfy setup/teardown
    # But shrinking tracking window is sufficient
    target_track.tracking_off = target_track.tracking_on + 1 
    target_track.end_time = target_track.tracking_off + (request.teardown_time * 60)
    
    corrupt_solution = Solution(tracks=corrupt_tracks)
    result = verify(instance, corrupt_solution)
    
    assert not result.is_valid
    assert any("below minimum" in e for e in result.errors)


def test_unknown_track_id(simple_valid_case):
    """Test detection of unknown track IDs."""
    instance, valid_solution = simple_valid_case
    
    corrupt_tracks = deepcopy(valid_solution.tracks)
    corrupt_tracks[0].track_id = "non-existent-uuid"
    
    corrupt_solution = Solution(tracks=corrupt_tracks)
    result = verify(instance, corrupt_solution)
    
    assert not result.is_valid
    assert any("Unknown track_id" in e for e in result.errors)


def test_invalid_antenna(simple_valid_case):
    """Test detection of execution on an invalid antenna."""
    instance, valid_solution = simple_valid_case
    
    corrupt_tracks = deepcopy(valid_solution.tracks)
    target_track = corrupt_tracks[0]
    
    # Change antenna to something that definitely doesn't exist
    target_track.resource = "DSS-999"
    
    corrupt_solution = Solution(tracks=corrupt_tracks)
    result = verify(instance, corrupt_solution)
    
    assert not result.is_valid
    assert any("Antenna 'DSS-999' not available" in e for e in result.errors)


@pytest.mark.parametrize("load_ground_truth", get_ground_truth_cases(), indirect=True)
def test_fairness_metric_calculation(load_ground_truth):
    """
    Test that the verifier correctly calculates fairness metrics (U_rms, U_max, per_mission_u_i).
    These metrics measure how evenly missions are satisfied.
    """
    instance, solution, metrics = load_ground_truth
    result = verify(instance, solution)

    # Check that fairness metrics are calculated and non-trivial
    assert hasattr(result, "u_rms")
    assert hasattr(result, "u_max")
    assert hasattr(result, "per_mission_u_i")

    # For ground truth solutions, we should have some missions with non-zero U_i
    # (random agent doesn't satisfy all requests)
    assert isinstance(result.per_mission_u_i, dict)
    assert len(result.per_mission_u_i) > 0, "Should have per-mission U_i values"

    # U_rms and U_max should be in valid range [0, 1]
    assert 0.0 <= result.u_rms <= 1.0, f"U_rms {result.u_rms} out of range [0, 1]"
    assert 0.0 <= result.u_max <= 1.0, f"U_max {result.u_max} out of range [0, 1]"
